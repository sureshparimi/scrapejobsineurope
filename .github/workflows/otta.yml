name: Scrape latest jobs

on:
  push:
  workflow_dispatch:
  schedule:
    - cron: '0 */4 * * *'
jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - name: Checking out repo
        uses: actions/checkout@v3
      - name: Setting up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'
      - name: Installing package list
        run: apt list --installed    
      - name: Removing previous chrome instances on runner 
        run: sudo apt purge google-chrome-stable  
        
       # Need to fetch reqs if needed
      - name: Installing all necessary packages
        run: pip install chromedriver-autoinstaller selenium pyvirtualdisplay
      - name: Install xvfb
        run: sudo apt-get install xvfb

      - name: Run the scraping script
        run: python scratchpad.py
      - name: Parse UniqueJobsdataMasterFile.json into table
        id: parse-json
        run: |
          echo "| Job Title | Job Location | Job Link | Job Posted |" > README.md
          echo "| --- | --- | --- | --- |" >> README.md
          jq -r '.[] | "| " + .job_title + " | " + .job_location + " | " + "[Apply](" + .JobLink + ")" + " | " + "$(date +'%e %B %Y')" + " |"' UniqueJobsdataMasterFile.json >> README.md
      - name: Commit and push if it changed
        run: |
          git config user.name "Automated"
          git config user.email "actions@users.noreply.github.com"
          git add README.md
          timestamp=$(date -u)
          git commit -m "Latest jobs: ${timestamp}" || exit 0
          git push
